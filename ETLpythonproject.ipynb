{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e23e079-8a2c-4a19-8927-53b143bd31d0",
   "metadata": {},
   "source": [
    "# Preparing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d2bca-adc3-42fb-8646-6f6a2983f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455de6fc-ab86-4b4e-b0b2-89e177e2c99c",
   "metadata": {},
   "source": [
    "# Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29269723-37c3-44fb-bb86-223babaeb63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevelRangeFilter(logging.Filter):\n",
    "    \"\"\"\n",
    "    A logging filter that allows messages within a specified range of logging levels to pass through.\n",
    "\n",
    "    Attributes:\n",
    "        min_level (int): The minimum logging level that the filter allows.\n",
    "        max_level (int): The maximum logging level that the filter allows.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_level, max_level):\n",
    "        \"\"\"\n",
    "        Initializes the filter with minimum and maximum logging levels.\n",
    "\n",
    "        Args:\n",
    "            min_level (int): The minimum logging level.\n",
    "            max_level (int): The maximum logging level.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.min_level = min_level\n",
    "        self.max_level = max_level\n",
    "\n",
    "    def filter(self, record):\n",
    "        \"\"\"\n",
    "        Determines if the specified record should be logged based on its level.\n",
    "\n",
    "        Args:\n",
    "            record (LogRecord): The log record to be checked.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the record's level is within the specified range, False otherwise.\n",
    "        \"\"\"\n",
    "        # Filter records that are not in the specified level range\n",
    "        return self.min_level <= record.levelno <= self.max_level\n",
    "\n",
    "\n",
    "# Create a logger for this module\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set the logging level for the logger\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# Setting the logger level to DEBUG means that all log messages, regardless of their severity, will be processed and output by this logger.\n",
    "\n",
    "# Create handlers\n",
    "success_handler = logging.FileHandler('etl_success.log')\n",
    "debug_handler = logging.FileHandler('etl_debug.log')\n",
    "error_handler = logging.FileHandler('etl_errors.log')\n",
    "console_handler = logging.StreamHandler()  # This handler will send logs to the console\n",
    "\n",
    "# Create formatters and add them to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "success_handler.setFormatter(formatter)\n",
    "debug_handler.setFormatter(formatter)\n",
    "error_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Create filters to the handlers\n",
    "success_filter = LevelRangeFilter(logging.INFO, logging.WARNING)\n",
    "error_filter = LevelRangeFilter(logging.WARNING, logging.CRITICAL)\n",
    "debug_filter = LevelRangeFilter(logging.DEBUG, logging.CRITICAL)\n",
    "console_filter = LevelRangeFilter(logging.INFO, logging.CRITICAL)\n",
    "\n",
    "# Add filters to the handlers\n",
    "success_handler.addFilter(success_filter)\n",
    "error_handler.addFilter(error_filter)\n",
    "debug_handler.addFilter(debug_filter)\n",
    "console_handler.addFilter(console_filter)\n",
    "\n",
    "# check if the logger currently has any handlers attached to it.\n",
    "# logger.handlers will return a list of handlers attached to logger\n",
    "if not logger.handlers:  #If this list is empty  it means no handlers are currently attached to the logger.\n",
    "    # Add handlers to the logger\n",
    "    logger.addHandler(success_handler)\n",
    "    logger.addHandler(debug_handler)\n",
    "    logger.addHandler(error_handler)\n",
    "    logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445718fb-878c-42a8-a4ab-65bbafcf53c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "logger.info('Logging is set up and ETL pipeline is going to be executed')\n",
    "#logger.debug('This is a debug message')\n",
    "#logger.warning('This is a warning message')\n",
    "#logger.error('This is an error message')\n",
    "#logger.critical('This is a critical message')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89276330-e07c-499c-9fd3-403b5f47884d",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5936d-99b8-4d1c-862b-2ed3b8fd6686",
   "metadata": {},
   "source": [
    "**API DOC**\n",
    "https://spoonacular.com/application/frontend/downloads/spoonacular-api-slides.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d8449-2468-4560-b8d5-799e54578d6a",
   "metadata": {},
   "source": [
    "**API Link** https://spoonacular.com/food-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd67cad-9f0a-4f16-957e-0bacef2a8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data (api_url, num_recipes=100):\n",
    "    \"\"\"\n",
    "    Extract data from the given API URL.\n",
    "\n",
    "    Parameters:\n",
    "    api_url (str): The URL of the API to extract data from.\n",
    "    num_recipes (int): The number of recipes to request from the API. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    dict or None: The data extracted from the API in JSON format, or None if there was an error.\n",
    "    \"\"\"\n",
    "    # Parameters to be sent with the API request\n",
    "    params = {'number': num_recipes}\n",
    "    try:\n",
    "        # Send a GET request to the API with the specified parameters\n",
    "        response = requests.get(api_url, params=params)\n",
    "        # Raise an exception for non-2xx status codes\n",
    "        response.raise_for_status() \n",
    "        try:\n",
    "            # Parse the response content as JSON\n",
    "            data = response.json()\n",
    "            return data\n",
    "        except json.JSONDecodeError as e:\n",
    "            # Log an error if JSON parsing fails\n",
    "            logger.error(f\"Error parsing JSON data: {e}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Log an error if the API request fails\n",
    "        logger.error(f\"Error extracting data from API: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6853909b-ac40-45bc-adb2-9482020ee5ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the API key used to authenticate with the Spoonacular API\n",
    "API_KEY = '9fe94823f081434989282d1622cfbc31'\n",
    "# Construct the API URL by appending the API key to the base URL for random recipes\n",
    "# The API_URL is composed of the base URL for the random recipes endpoint and the API key as a query parameter\n",
    "API_URL = 'https://api.spoonacular.com/recipes/random?apiKey=' + API_KEY\n",
    "\n",
    "# Call the extract_data function to fetch data from the API URL\n",
    "# The extract_data function sends a GET request to the API_URL and returns the response data\n",
    "data = extract_data(API_URL)\n",
    "\n",
    "# Check if data was successfully extracted\n",
    "if data:\n",
    "    logger.debug(data) # Log the extracted data at the debug level\n",
    "    logger.info(\"Data extracted successfully.\") # Log a success message at the info level\n",
    "else:\n",
    "    # Log an error message if data extraction failed\n",
    "    logger.error(\"An error occurred while fetching data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e55ca4f-a268-4bb9-8190-1164638d42cf",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c7074-412a-4f7c-94d9-fa87386d3326",
   "metadata": {},
   "source": [
    "## Json to Pandas\n",
    "you can import any json data into pandas, the keys will become columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ed4c5-0cf3-4758-b383-d53a7c05aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the 'recipes' key exists in the data dictionary and contains a list\n",
    "if 'recipes' in data and isinstance(data['recipes'], list):\n",
    "    # Convert the list of recipe dictionaries into a pandas DataFrame\n",
    "    df = pd.DataFrame(data['recipes'])\n",
    "    \n",
    "    # Display the first few rows of the DataFrame to verify its structure\n",
    "    logger.debug(df.head())\n",
    "    \n",
    "    # Log the number of recipes extracted\n",
    "    logger.info(f\"Extracted {len(df)} recipes.\")\n",
    "\n",
    "    # Log detailed information about the DataFrame, such as column types and non-null counts\n",
    "    logger.debug(df.info())\n",
    "\n",
    "else:\n",
    "    # Log an error if the 'recipes' key is missing or does not contain a list\n",
    "    logger.error(\"'recipes' key is missing or does not contain a valid list in the data dictionary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33db2a9-9efe-46f0-885d-81693b299b45",
   "metadata": {},
   "source": [
    "## Dealing with recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1d8cb-2308-4a75-8b3b-96ba4b098577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the required columns for the dataframe\n",
    "dfrecipes_COLUMNS = [\n",
    "    'vegetarian', 'vegan', 'glutenFree', 'dairyFree', 'veryHealthy', \n",
    "    'cheap', 'veryPopular', 'sustainable', 'lowFodmap', 'pricePerServing', \n",
    "    'title', 'readyInMinutes', 'servings', 'sourceUrl', 'summary', 'license'\n",
    "]\n",
    "# Construct the dataframe\n",
    "try:\n",
    "    dfrecipes = pd.DataFrame(df, columns=dfrecipes_COLUMNS)\n",
    "    logger.info(f\"Constructed DataFrame with {len(dfrecipes)} recipes.\")\n",
    "    logger.debug(dfrecipes.head())\n",
    "    logger.debug(dfrecipes.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to construct DataFrame:\", exc_info=True)\n",
    "\n",
    "\n",
    "# Find duplicates based on the 'title' column\n",
    "try:\n",
    "    duplicates = dfrecipes[dfrecipes.duplicated(subset=['title'])]\n",
    "    logger.debug(duplicates)\n",
    "    if not duplicates.empty:\n",
    "    # remove duplicates\n",
    "        logger.warning(f\"Found {len(duplicates)} duplicate recipes. Removing duplicates.\")\n",
    "        dfrecipes = dfrecipes.drop_duplicates(subset=['title'])\n",
    "        dfrecipes.reset_index(drop=True, inplace=True)\n",
    "        logger.info(f\"DataFrame now contains {len(dfrecipes)} recipes after removing duplicates.\")\n",
    "    else:\n",
    "        logger.info(\"No duplicates found\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to handle duplicates\", exc_info=True)\n",
    "\n",
    "\n",
    "# Rename columns for clarity and consistency\n",
    "COLUMNS_TO_RENAME_MAP={\n",
    "    'vegetarian': 'is_vegetarian', 'vegan': 'is_vegan',\n",
    "    'glutenFree': 'is_glutenFree', 'dairyFree': 'is_dairyFree',\n",
    "    'veryHealthy': 'is_healthy', 'cheap': 'is_cheap',\n",
    "    'veryPopular': 'is_Popular', 'sustainable': 'is_sustainable',\n",
    "    'lowFodmap': 'is_lowFodmap', 'pricePerServing': 'price_per_serving', \n",
    "    'readyInMinutes': 'ready_min', 'sourceUrl': 'source_url', \n",
    "    'title': 'recipe_title'}\n",
    "try:\n",
    "    dfrecipes = dfrecipes.rename(columns=COLUMNS_TO_RENAME_MAP)\n",
    "    logger.info(\"Renamed columns.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to rename columns\", exc_info=True)\n",
    "\n",
    "# Generate successive numbers for the 'id_recipe' column\n",
    "try:\n",
    "    dfrecipes['id_recipe'] = range(1, len(dfrecipes) + 1)\n",
    "    logger.info(\"Added 'id_recipe'.\")\n",
    "    logger.debug(dfrecipes.head())\n",
    "    logger.debug(dfrecipes.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to add 'id_recipe' column\", exc_info=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed86ad-77d4-44c5-9e89-1af59198febd",
   "metadata": {},
   "source": [
    "## Dealing with ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206eb4f-466d-4a92-8098-903b4d7d76f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the required columns for the ingredients DataFrame\n",
    "dfIng_COLUMNS=['consistency', 'nameClean', 'aisle', 'name']  \n",
    "\n",
    "\n",
    "# Define a function to a list of dictionaries\n",
    "def extract_dicts(lst):\n",
    "    \"\"\"Extract dictionaries from a list of dictionaries and return a list\"\"\"\n",
    "    return [d for d in lst if isinstance(d, dict)]\n",
    "# Apply the function and sum the results\n",
    "try:\n",
    "    # 'all_ingredients' variable recives a list of all dictionaries in the extendedIngredients column\n",
    "    # note that a dataframe row is a dictionary\n",
    "    all_ingredients = df['extendedIngredients'].apply(extract_dicts).sum()\n",
    "    logger.info(f\"Extracted {len(all_ingredients)} ingredients.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to extract ingredient dictionaries\", exc_info=True)\n",
    "    all_ingredients = []\n",
    "\n",
    "# Construct the ingredients DataFrame\n",
    "try:\n",
    "    dfIng = pd.DataFrame(all_ingredients, columns=dfIng_COLUMNS)\n",
    "    logger.info(f\"Constructed ingredients DataFrame with {len(dfIng)} ingredients.\")\n",
    "    logger.debug(dfIng.head())\n",
    "    logger.debug(dfIng.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to construct ingredients DataFrame\", exc_info=True)\n",
    "    dfIng = pd.DataFrame(columns=dfIng_COLUMNS)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Find duplicates\n",
    "    duplicates = dfIng[dfIng.duplicated()]\n",
    "    logger.debug(duplicates)\n",
    "    # Remove duplicates\n",
    "    if not duplicates.empty:\n",
    "        logger.warning(f\"Found {len(duplicates)} duplicate ingredients. Removing duplicates.\")\n",
    "        dfIng = dfIng.drop_duplicates().reset_index(drop=True)\n",
    "        logger.info(f\"DataFrame now contains {len(dfIng)} ingredients after removing duplicates.\")\n",
    "    else:\n",
    "        logger.info(\"No duplicates found.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to handle duplicates\", exc_info=True)\n",
    "\n",
    "# Handle missing values in the nameClean column\n",
    "try:\n",
    "    none_rows = dfIng[dfIng['nameClean'].isna()]\n",
    "    logger.debug(none_rows)\n",
    "    if not none_rows.empty:\n",
    "        logger.warning(f\"Found {len(none_rows)} null nameClean. Filling with name.\")\n",
    "        logger.debug(none_rows)\n",
    "        dfIng['nameClean'] = dfIng['nameClean'].fillna(dfIng['name'])\n",
    "        logger.info(\"Filled missing 'nameClean' values.\")\n",
    "    else:\n",
    "        logger.info(\"No null 'nameClean' found.\")\n",
    "    logger.debug(dfIng.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to handle null values in 'nameClean'\", exc_info=True)\n",
    "\n",
    "# Drop the 'name' column after filling 'nameClean'\n",
    "try:\n",
    "    dfIng.drop('name', axis=1, inplace=True)\n",
    "    logger.info(\"Dropped 'name' column.\")\n",
    "    logger.debug(dfIng.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to drop 'name' column\", exc_info=True)\n",
    "\n",
    "try:\n",
    "    # Find duplicates\n",
    "    duplicates = dfIng[dfIng.duplicated()]\n",
    "    logger.debug(duplicates)\n",
    "    # Remove duplicates\n",
    "    if not duplicates.empty:\n",
    "        logger.warning(f\"Found {len(duplicates)} duplicate ingredients. Removing duplicates.\")\n",
    "        dfIng = dfIng.drop_duplicates().reset_index(drop=True)\n",
    "        logger.info(f\"DataFrame now contains {len(dfIng)} ingredients after removing duplicates.\")\n",
    "    else:\n",
    "        logger.info(\"No duplicates found.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to handle duplicates\", exc_info=True)\n",
    "\n",
    "# Rename 'nameClean' to 'ing_name'\n",
    "try:\n",
    "    dfIng = dfIng.rename(columns={'nameClean': 'ing_name'})\n",
    "    logger.info(\"Renamed 'nameClean' to 'ing_name'.\")\n",
    "    logger.debug(dfIng.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to rename columns\", exc_info=True)\n",
    "\n",
    "# Generate 'id_ingredient' column\n",
    "try:\n",
    "    dfIng['id_ingredient'] = range(1, len(dfIng) + 1)\n",
    "    logger.info(\"Added 'id_ingredient'.\")\n",
    "    logger.debug(dfIng.head())\n",
    "    logger.debug(dfIng.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to add 'id_ingredient' column\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84878a76-16aa-4d8f-be5c-7c9f91e6ccd1",
   "metadata": {},
   "source": [
    "## Dealing with measures (reference_ing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b939b141-ed1e-4c53-9d2b-e1d8be3c03a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the column that contains all the ingredients measures along with the recipes titles\n",
    "try:\n",
    "    dfrecipeIng = pd.DataFrame(df, columns=['extendedIngredients', 'title'])\n",
    "    logger.info(f\"Extracted ingredients and titles for {len(dfrecipeIng)} recipes.\")\n",
    "    logger.debug(dfrecipeIng.head())\n",
    "    logger.debug(dfrecipeIng.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to extract ingredients and titles\", exc_info=True)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Obtaining a new dataframe that contains measures of ingredients in each recipe\n",
    "\n",
    "# Define the required columns for the measures DataFrame\n",
    "dfmeasures_COLUMNS =['nameClean', 'name', 'measures', 'title']\n",
    "\n",
    "# Initialize an empty list to collect rows of data\n",
    "rows_data = []\n",
    "\n",
    "# Iterate through each row of the original DataFrame\n",
    "for i, row_series in dfrecipeIng.iterrows():\n",
    "    try:\n",
    "        # Access the recipe title for the current row \n",
    "        title = row_series['title']  # title is a string\n",
    "        \n",
    "        # Access the 'extendedIngredients' column for the current row\n",
    "        recipe_ingredients_list = row_series['extendedIngredients']  # recipe_ingredients_list is a list\n",
    "    \n",
    "        # if recipe_ingredients_list is not an empty list skip the iteration (according to the Guard clauses principle of clean code)\n",
    "        if not (recipe_ingredients_list and isinstance(recipe_ingredients_list, list)):\n",
    "            continue\n",
    "            \n",
    "        # Iterate through each dictionary in the list\n",
    "        for each_dict in recipe_ingredients_list:\n",
    "            if each_dict and isinstance(each_dict, dict):  # Check if each_dict is non-empty and is a dictionary\n",
    "                # Get measures dict \n",
    "                measures_dict = each_dict.get('measures', {})\n",
    "                # Transform measures dict to a list\n",
    "                measures_list = [measures_dict]       \n",
    "            else:\n",
    "                measures_list = None\n",
    "            \n",
    "            # Create a dictionary for the row data\n",
    "            row_data = {\n",
    "                'nameClean': each_dict.get('nameClean', None),\n",
    "                'measures': measures_list,\n",
    "                'name': each_dict.get('name'),\n",
    "                'title': title\n",
    "            }\n",
    "            \n",
    "            # Append the row data to the list\n",
    "            rows_data.append(row_data)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing row {i} for recipe '{title}'\", exc_info=True)\n",
    "\n",
    "# Create the DataFrame from the list of row_data\n",
    "try:\n",
    "    dfmeasures = pd.DataFrame(rows_data, columns=dfmeasures_COLUMNS)\n",
    "    logger.info(f\"Constructed measures DataFrame with {len(dfmeasures)} rows.\")\n",
    "    logger.debug(dfmeasures.head())\n",
    "    logger.debug(dfmeasures.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to construct measures DataFrame\", exc_info=True)\n",
    "\n",
    "# Handle missing values in the nameClean column\n",
    "try:\n",
    "    none_rows = dfmeasures[dfmeasures['nameClean'].isna()]\n",
    "    logger.debug(none_rows)\n",
    "    if not none_rows.empty:\n",
    "        logger.warning(f\"Found {len(none_rows)} null nameClean. Filling with name.\")\n",
    "        dfmeasures['nameClean'] = dfmeasures['nameClean'].fillna(dfmeasures['name'])\n",
    "        logger.info(\"Filled missing 'nameClean' values.\")\n",
    "    else:\n",
    "        logger.info(\"No null 'nameClean' found.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to handle null values in 'nameClean'\", exc_info=True)\n",
    "\n",
    "# Drop the 'name' column after filling 'nameClean'\n",
    "try:\n",
    "    dfmeasures.drop('name', axis=1, inplace=True)\n",
    "    logger.info(\"Dropped 'name' column.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to drop 'name' column\", exc_info=True)\n",
    "\n",
    "# Rename 'nameClean' to 'ing_name'\n",
    "try:\n",
    "    dfmeasures = dfmeasures.rename(columns={'nameClean': 'ing_name'})\n",
    "    logger.info(\"Renamed 'nameClean' to 'ing_name'.\")\n",
    "    logger.debug(dfmeasures.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to rename columns\", exc_info=True)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "#transforming the measures column\n",
    "\n",
    "# Function to extract measure information from measures dict\n",
    "def extract_measure(measures_dict, key_name):\n",
    "  \"\"\"\n",
    "  Extracts a string value representing the measure from the given dictionary using a given name.\n",
    "  Handles cases where the key name might be different.\n",
    "\n",
    "  Args:\n",
    "      measures_dict: A dictionary containing the measure information.\n",
    "      key_name: The key name to look for.\n",
    "\n",
    "  Returns:\n",
    "      A string representing the measure in the format \"amount unitShort\".\n",
    "  \"\"\"\n",
    "  if key_name in measures_dict:\n",
    "    return f\"{measures_dict[key_name]['amount']} {measures_dict[key_name]['unitShort']}\"\n",
    "  else:\n",
    "    # Handle cases where the key might be different\n",
    "    for key in measures_dict:\n",
    "      if isinstance(measures_dict[key], dict):\n",
    "        return extract_measure(measures_dict[key], key)\n",
    "    # If no matching key is found, return an empty string\n",
    "    return None\n",
    "\n",
    "# Create two new columns with extracted measures\n",
    "try:\n",
    "    dfmeasures['measure_1'] = dfmeasures['measures'].apply(lambda x: extract_measure(x[0], \"us\"))\n",
    "    dfmeasures['measure_2'] = dfmeasures['measures'].apply(lambda x: extract_measure(x[0], \"metric\"))\n",
    "    logger.info(\"Created columns 'measure_1' and 'measure_2'.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to create 'measure_1' and 'measure_2' columns\", exc_info=True)\n",
    "\n",
    "# Drop the original 'measures' column\n",
    "try:\n",
    "    dfmeasures.drop('measures', axis=1, inplace=True)\n",
    "    logger.info(\"Dropped 'measures' column.\")\n",
    "    logger.debug(dfmeasures.head())\n",
    "    logger.debug(dfmeasures.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to drop 'measures' column\", exc_info=True)\n",
    "\n",
    "# Function to combine measures from 'measure_1' and 'measure_2'\n",
    "def combine_measures(row):\n",
    "  \"\"\"\n",
    "  Combines values from 'measure_1' and 'measure_2' columns into a single string.\n",
    "\n",
    "  Args:\n",
    "      row: A pandas Series representing a row of the DataFrame.\n",
    "\n",
    "  Returns:\n",
    "      A string containing the combined measure value(s).\n",
    "  \"\"\"\n",
    "  measure_1 = row['measure_1']\n",
    "  measure_2 = row['measure_2']\n",
    "\n",
    "  if measure_1 == measure_2:\n",
    "    return measure_1  # Same values, return one\n",
    "  else:\n",
    "    return f\"{measure_1} / {measure_2}\"  # Different values are concatenated with \"/\"\n",
    "\n",
    "# Apply the function to create a new 'measure' column\n",
    "try:\n",
    "    dfmeasures['measure'] = dfmeasures.apply(combine_measures, axis=1)\n",
    "    logger.info(\"Created 'measure' column by combining 'measure_1' and 'measure_2'.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to create 'measure' column\", exc_info=True)\n",
    "\n",
    "# Drop 'measure_1' and 'measure_2' columns\n",
    "try:\n",
    "    dfmeasures.drop(['measure_1', 'measure_2'], axis=1, inplace=True)\n",
    "    logger.info(\"Dropped 'measure_1' and 'measure_2' columns.\")\n",
    "    logger.debug(dfmeasures.head())\n",
    "    logger.debug(dfmeasures.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to drop 'measure_1' and 'measure_2' columns\", exc_info=True)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# dealing with id_recipe in dfmeasures\n",
    "\n",
    "# Create a mapping dictionary from dfrecipes, \n",
    "try:\n",
    "    RECIPE_MAPPING = dfrecipes.set_index('recipe_title')['id_recipe'].to_dict()\n",
    "    logger.info(\"Created 'RECIPE_MAPPING'.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to create 'RECIPE_MAPPING'\", exc_info=True)\n",
    "\n",
    "# Function to map recipe titles to id_recipe\n",
    "def map_with_none(recipe_title, RECIPE_MAPPING):\n",
    "    \"\"\"Maps recipe names to id_recipe, handling missing values.\"\"\"\n",
    "    if pd.isna(recipe_title):\n",
    "        return None  # Return None for missing titles\n",
    "    return RECIPE_MAPPING.get(recipe_title, None)  # Use get() to avoid KeyError for missing keys\n",
    "\n",
    "try:\n",
    "    # Apply the mapping function to create 'id_recipe' column\n",
    "    dfmeasures['id_recipe'] = dfmeasures['title'].apply(map_with_none, args=(RECIPE_MAPPING,))\n",
    "    logger.info(\"Added 'id_recipe' column based on 'RECIPE_MAPPING'.\")\n",
    "    # Convert id_recipe to integer type where applicable, keeping None values\n",
    "    dfmeasures['id_recipe'] = dfmeasures['id_recipe'].astype('Int64')\n",
    "    logger.info(\"Converted 'id_recipe' column to Integer type.\")\n",
    "    logger.debug(dfmeasures.head())\n",
    "    logger.debug(dfmeasures.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to add or convert 'id_recipe' column\", exc_info=True)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# dealing with id_ingredient in dfmeasures\n",
    "\n",
    "# Create a mapping dictionary from dfIng\n",
    "try:\n",
    "    ING_MAPPING = dfIng.set_index('ing_name')['id_ingredient'].to_dict()\n",
    "    logger.info(\"Created 'ING_MAPPING'.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to create 'ING_MAPPING'\", exc_info=True)\n",
    "\n",
    "# Function to map ingredient names to id_ingredient\n",
    "def map_with_none(ing_name, ING_MAPPING):\n",
    "    \"\"\"Maps ingredient names to id_ingredient, handling missing values.\"\"\"\n",
    "    if pd.isna(ing_name):\n",
    "        return None  # Return None for missing titles\n",
    "    return ING_MAPPING.get(ing_name, None)  # Use get() to avoid KeyError for missing keys\n",
    "    \n",
    "try:\n",
    "    # Apply the mapping function to create 'id_ingredient' column\n",
    "    dfmeasures['id_ingredient'] = dfmeasures['ing_name'].apply(map_with_none, args=(ING_MAPPING,))\n",
    "    logger.info(\"Added 'id_ingredient' column based on 'ING_MAPPING'.\")\n",
    "    # Convert id_ingredient to integer type where applicable, keeping None values\n",
    "    dfmeasures['id_ingredient'] = dfmeasures['id_ingredient'].astype('Int64')\n",
    "    logger.info(\"Converted 'id_ingredient' column to Integer type.\")\n",
    "    logger.debug(dfmeasures.head())\n",
    "    logger.debug(dfmeasures.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to add or convert 'id_ingredient' column\", exc_info=True)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Drop 'ing_name' and 'title' columns to finalize the reference DataFrame\n",
    "try:\n",
    "    dfreference_ing = dfmeasures.drop(['ing_name', 'title'], axis=1)\n",
    "    logger.info(\"Dropped 'ing_name' and 'title' columns.\")\n",
    "    logger.debug(dfreference_ing.head())\n",
    "    logger.debug(dfreference_ing.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to finalize reference DataFrame by dropping 'ing_name' and 'title' columns\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b3112f-2865-448a-8892-80ea54f40dc0",
   "metadata": {},
   "source": [
    "## Dealing with steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0006d-7e9c-4597-bc5e-eee6adeab69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the column that contains all the instructions along with the recipes titles\n",
    "try:\n",
    "    dfAllIns = pd.DataFrame(df, columns=['analyzedInstructions', 'title'])\n",
    "    logger.info(f\"Extracted instructions for {len(dfAllIns)} recipes.\")\n",
    "    logger.debug(dfAllIns.head())\n",
    "    logger.debug(dfAllIns.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to extract instructions and titles\", exc_info=True)\n",
    "\n",
    "#---------------------------------------------------------\n",
    "# obtaining a new dataframe that contains steps of each recipe\n",
    "\n",
    "\n",
    "# Define the required columns for the steps DataFrame\n",
    "dfsteps_COLUMNS = ['steps', 'title']\n",
    "\n",
    "# Initialize an empty list to collect rows of data\n",
    "rows_data = []\n",
    "\n",
    "# Define the required columns for the steps DataFrame\n",
    "for i, row_series in dfAllIns.iterrows():\n",
    "    try:\n",
    "        # Access the recipe title for the current row \n",
    "        title = row_series['title']  # title is a string\n",
    "        \n",
    "        # Access the 'analyzedInstructions' column for the current row\n",
    "        instructions_list = row_series['analyzedInstructions']  # instructions_list is a list containing one dictionary\n",
    "        \n",
    "        if instructions_list and isinstance(instructions_list[0], dict):  # Check if instructions_list is non-empty and its first element is a dictionary\n",
    "            # Get steps list\n",
    "            steps_list = instructions_list[0].get('steps', [])\n",
    "        else:\n",
    "            steps_list = None\n",
    "        \n",
    "        # Create a dictionary for the row data\n",
    "        row_data = {\n",
    "            'steps': steps_list,\n",
    "            'title': title\n",
    "        }\n",
    "        \n",
    "        # Append the row data to the list\n",
    "        rows_data.append(row_data)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing row {i} for recipe '{title}'\", exc_info=True)\n",
    "\n",
    "# Create the DataFrame from the list of row data\n",
    "try:\n",
    "    dfsteps = pd.DataFrame(rows_data, columns=dfsteps_COLUMNS)\n",
    "    logger.info(f\"Constructed steps DataFrame for {len(dfsteps)} recipes.\")\n",
    "    logger.debug(dfsteps.head())\n",
    "    logger.debug(dfsteps.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to construct steps DataFrame\", exc_info=True)\n",
    "\n",
    "\n",
    "#---------------------------------------------------------\n",
    "# transforming the steps column\n",
    "\n",
    "# Define the required columns for the step DataFrame\n",
    "dfsteps_COLUMNS = ['equipment', 'step', 'length', 'number', 'title']\n",
    "\n",
    "# Initialize an empty list to collect rows of data\n",
    "rows_data = []\n",
    "\n",
    "# Iterate through each row of the original DataFrame dfsteps\n",
    "for i, row_series in dfsteps.iterrows():\n",
    "    try:\n",
    "        # Access the recipe title for the current row \n",
    "        title = row_series['title']  # title is a string\n",
    "        \n",
    "        # Access the 'steps' column for the current row\n",
    "        steps_list = row_series['steps']\n",
    "        \n",
    "        if steps_list is not None:\n",
    "            # Iterate through each dictionary in the steps_list\n",
    "            for each_dict in steps_list:\n",
    "                if each_dict and isinstance(each_dict, dict):  # Check if each_dict is non-empty and is a dictionary\n",
    "                    # Extract relevant information from each_dict\n",
    "                    number = each_dict.get('number', None)\n",
    "                    step = each_dict.get('step', None)\n",
    "                    time = each_dict.get('length', {}).get('number', None)\n",
    "                    unit = each_dict.get('length', {}).get('unit', None)\n",
    "                    \n",
    "                    # Calculate length based on time and unit\n",
    "                    if time is None or unit is None:\n",
    "                        length = None\n",
    "                    else:\n",
    "                        length = f\"{time} {unit}\"\n",
    "                    \n",
    "                    equipment_list = each_dict.get('equipment', [])  # Get equipment list or empty list if 'equipment' is missing\n",
    "                    \n",
    "                    # Handle NaN or None values in equipment_list\n",
    "                    if isinstance(equipment_list, list):\n",
    "                        equipment_list = [e if pd.notna(e) else None for e in equipment_list]\n",
    "                    \n",
    "                    # Create a dictionary for the row data\n",
    "                    row_data = {\n",
    "                        'length': length,\n",
    "                        'number': number,\n",
    "                        'step': step,\n",
    "                        'equipment': equipment_list,\n",
    "                        'title': title\n",
    "                    }\n",
    "                    \n",
    "                    # Append the row data to the list\n",
    "                    rows_data.append(row_data)\n",
    "        else:\n",
    "            # If steps_list is None, create a row with None values\n",
    "            row_data = {\n",
    "                'length': None,\n",
    "                'number': None,\n",
    "                'step': None,\n",
    "                'equipment': None,\n",
    "                'title': title\n",
    "            }\n",
    "            \n",
    "            # Append the row data to the list\n",
    "            rows_data.append(row_data)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing row {i} for recipe '{title}'\", exc_info=True)\n",
    "\n",
    "# Create the DataFrame from the list of row data\n",
    "try:\n",
    "    dfstep = pd.DataFrame(rows_data, columns=dfsteps_COLUMNS)\n",
    "    logger.info(f\"Constructed steps DataFrame with {len(dfstep)} steps.\")\n",
    "    logger.debug(dfstep.head())\n",
    "    logger.debug(dfstep.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to construct steps DataFrame\", exc_info=True)\n",
    "\n",
    "# Find duplicates\n",
    "try:\n",
    "    duplicates = dfstep[dfstep.duplicated(subset=[\"step\", \"length\", \"number\", \"title\"])]\n",
    "    logger.debug(duplicates)\n",
    "\n",
    "    # Remove duplicates\n",
    "    if not duplicates.empty:\n",
    "        logger.warning(f\"Found {len(duplicates)} duplicate steps. Removing duplicates.\")\n",
    "        dfstep = dfstep.drop_duplicates()\n",
    "        dfstep.reset_index(drop=True, inplace=True)\n",
    "        logger.info(f\"DataFrame now contains {len(dfstep)} steps after removing duplicates.\")\n",
    "    else:\n",
    "        logger.info(\"No duplicates found\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to find or remove duplicates\", exc_info=True)\n",
    "\n",
    "# Checking if there's a Null values in step column\n",
    "try:\n",
    "    none_rows = dfstep[dfstep['step'].isna()]\n",
    "    logger.debug(none_rows)\n",
    "\n",
    "    # Deleting the null steps in dfstep DataFrame.\n",
    "    if len(none_rows) > 0:\n",
    "        logger.warning(f\"Found {len(none_rows)} null steps. Deleting null steps.\")\n",
    "        dfstep.dropna(subset=['step'], inplace=True)\n",
    "        dfstep.reset_index(drop=True, inplace=True)\n",
    "        logger.info(\"Null steps deleted\")\n",
    "        logger.info(f\"DataFrame now contains {len(dfstep)} steps after removing null values.\")\n",
    "    else:\n",
    "        logger.info(\"No null steps found\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to handle null steps\", exc_info=True)\n",
    "\n",
    "logger.debug(dfstep.head())\n",
    "logger.debug(dfstep.info())   \n",
    "    \n",
    "#---------------------------------------------------------\n",
    "#transforming equipment column\n",
    "\n",
    "# Initialize an empty list to collect rows of data\n",
    "rows_data = []\n",
    "\n",
    "# Iterate through each row of the original DataFrame dfstep\n",
    "for i, row_series in dfstep.iterrows():\n",
    "    try:\n",
    "        # Access the recipe title for the current row \n",
    "        title = row_series['title']\n",
    "        # Access the step length for the current row \n",
    "        length = row_series['length']\n",
    "        # Access the recipe step for the current row \n",
    "        step = row_series['step']\n",
    "        # Access the step number for the current row \n",
    "        number = row_series['number']\n",
    "        # Access the 'equipment' column for the current row\n",
    "        equipments_list = row_series['equipment']  #\n",
    "        \n",
    "        # Initialize variables to store processed data\n",
    "        equipments_name_list = []\n",
    "        \n",
    "        # Process equipments_list\n",
    "        if equipments_list and isinstance(equipments_list, list):\n",
    "            for each_dict in equipments_list:\n",
    "                if each_dict and isinstance(each_dict, dict):  # Check if each_dict is non-empty and is a dictionary\n",
    "                    # Get a list containing equipments names\n",
    "                    equipment_name = each_dict.get('name', None)\n",
    "                    if equipment_name is not None:  # Ensure equipment_name is not None\n",
    "                        equipments_name_list.append(equipment_name)\n",
    "        else:\n",
    "            equipments_name_list = None  # Handle case where equipments_list is None or not a list\n",
    "        \n",
    "        # Create a dictionary for the row data\n",
    "        row_data = {\n",
    "            'length': length,\n",
    "            'number': number,\n",
    "            'step': step,\n",
    "            'equipment': equipments_name_list,\n",
    "            'title': title\n",
    "        }\n",
    "        \n",
    "        # Append the row data to the list\n",
    "        rows_data.append(row_data)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing row {i} for recipe '{title}'\", exc_info=True)\n",
    "\n",
    "# Create the DataFrame from the list of row data\n",
    "try:\n",
    "    dfstepclean = pd.DataFrame(rows_data, columns=dfsteps_COLUMNS)\n",
    "    logger.info(f\"Constructed dfstepclean DataFrame with {len(dfstepclean)} steps and contains cleaned lists of equipments\")\n",
    "    logger.debug(dfstepclean.head())\n",
    "    logger.debug(dfstepclean.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to construct dfstepclean DataFrame\", exc_info=True)\n",
    "\n",
    "#---------------------------------------------------------\n",
    "#dealing with dfstep_final\n",
    "\n",
    "try:\n",
    "    # generating id_step\n",
    "    dfstepclean['id_step'] = range(1, len(dfstepclean) + 1)\n",
    "    logger.info(\"Added 'id_step'.\")\n",
    "    logger.debug(dfstepclean.info())\n",
    "\n",
    "    dfstep_final = dfstepclean.copy()\n",
    "    logger.debug(dfstep_final.info())\n",
    "\n",
    "    # Apply the custom map function\n",
    "    dfstep_final['id_recipe'] = dfstep_final['title'].apply(map_with_none, args=(RECIPE_MAPPING,))\n",
    "    logger.info(\"'id_recipe' column added according to the 'RECIPE_MAPPING'\")\n",
    "\n",
    "    # Convert id_recipe to integer type where applicable, keeping None values\n",
    "    dfstep_final['id_recipe'] = dfstep_final['id_recipe'].astype('Int64')\n",
    "    logger.info(\"'id_recipe' column converted to Integer\")\n",
    "\n",
    "    # Drop columns by label (column name)\n",
    "    dfstep_final.drop(columns=['equipment', 'title'], inplace=True)\n",
    "    logger.info(\"'equipment' and 'title' columns deleted\")\n",
    "    logger.debug(dfstep_final.head())\n",
    "    logger.debug(dfstep_final.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to finalize dfstep_final DataFrame\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372e34e-ddde-4147-a0e5-3ce9dd7289dd",
   "metadata": {},
   "source": [
    "## Dealing with instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a0c3c-5d8b-43c6-b039-df9c57b6ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Assign a unique number to each unique title\n",
    "    dfstep_final['instruction_id'] = pd.factorize(dfstep_final['id_recipe'])[0] + 1\n",
    "    logger.info(\"Added 'instruction_id'.\")\n",
    "    logger.debug(dfstep_final.head(30))\n",
    "    logger.debug(dfstep_final.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to add 'instruction_id'.\", exc_info=True)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create the instructions dataframe\n",
    "    dfIns = dfstep_final[['instruction_id', 'id_recipe']].copy()\n",
    "    logger.info(\"Constructed instructions dataframe\")\n",
    "    logger.debug(dfIns.head())\n",
    "    logger.debug(dfIns.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to construct instructions dataframe\", exc_info=True)\n",
    "\n",
    "try:\n",
    "    # Find duplicates\n",
    "    duplicates = dfIns[dfIns.duplicated()]\n",
    "    logger.debug(duplicates)\n",
    "\n",
    "    # Remove duplicates\n",
    "    if not duplicates.empty:\n",
    "        logger.warning(f\"Found {len(duplicates)} duplicate instructions. Removing duplicates.\")\n",
    "        dfIns = dfIns.drop_duplicates()\n",
    "        dfIns.reset_index(drop=True, inplace=True)\n",
    "        logger.info(f\"DataFrame now contains {len(dfIns)} instructions after removing duplicates.\")\n",
    "    else:\n",
    "        logger.info(\"No duplicates found\")\n",
    "except Exception as e:\n",
    "    logger.error(\"An unexpected error occurred while handling duplicates.\", exc_info=True)\n",
    "\n",
    "try:\n",
    "    # Drop a column by label (column name)\n",
    "    dfstep_final.drop(columns=['id_recipe'], inplace=True)\n",
    "    logger.info(\"'id_recipe' deleted from steps dataframe\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to drop 'id_recipe' column.\", exc_info=True)\n",
    "\n",
    "logger.debug(dfIns.head())\n",
    "logger.debug(dfIns.info())\n",
    "logger.debug(dfstep_final.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f0707-6c48-434d-8a0f-91e21808971b",
   "metadata": {},
   "source": [
    "## Dealing with equipments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c06e73-8208-4bbc-8439-964fd8eccaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the required columns for the equipments DataFrame\n",
    "dfequip_COLUMNS = ['name']\n",
    "\n",
    "# Initialize an empty list to collect rows of data\n",
    "rows_data = []\n",
    "\n",
    "# Iterate through each row of the original DataFrame\n",
    "for i, row_series in dfstep.iterrows():\n",
    "    try:\n",
    "        equip_list = row_series['equipment']\n",
    "        \n",
    "        if equip_list and isinstance(equip_list, list):\n",
    "            for each_dict in equip_list:\n",
    "                # Check if the element is a dictionary\n",
    "                if each_dict and isinstance(each_dict, dict):\n",
    "                    name = each_dict.get('name', None)\n",
    "                    \n",
    "                    # Create a dictionary for the row data\n",
    "                    row_data = {\n",
    "                        'name': name\n",
    "                    }\n",
    "                    \n",
    "                    # Append the row data to the list\n",
    "                    rows_data.append(row_data)\n",
    "                else:\n",
    "                    logger.warning(f\"Non-dictionary element found in equipment list at row {i}. Skipping element.\")\n",
    "        else:\n",
    "            logger.warning(f\"Equipment list is empty or not a list at row {i}. Skipping row.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing row {i}\", exc_info=True)\n",
    "\n",
    "# Create the DataFrame from the list of row data\n",
    "try:\n",
    "    dfequip = pd.DataFrame(rows_data, columns=dfequip_COLUMNS)\n",
    "    logger.info(f\"Constructed equipments DataFrame with {len(dfequip)} equipments.\")\n",
    "    logger.debug(dfequip.head())\n",
    "    logger.debug(dfequip.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to construct equipments DataFrame.\", exc_info=True)\n",
    "    dfequip = pd.DataFrame(columns=dfequip_COLUMNS)  # Create an empty DataFrame as a fallback\n",
    "\n",
    "\n",
    "try:\n",
    "    # Find duplicates\n",
    "    duplicates = dfequip[dfequip.duplicated()]\n",
    "    logger.debug(duplicates)\n",
    "\n",
    "    # Remove duplicates\n",
    "    if not duplicates.empty:\n",
    "        logger.warning(f\"Found {len(duplicates)} duplicate equipments. Removing duplicates.\")\n",
    "        dfequip = dfequip.drop_duplicates()\n",
    "        dfequip.reset_index(drop=True, inplace=True)\n",
    "        logger.info(f\"DataFrame now contains {len(dfequip)} equipments after removing duplicates.\")\n",
    "    else:\n",
    "        logger.info(\"No duplicates found\")\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred while finding or removing duplicates.\", exc_info=True)\n",
    "\n",
    "\n",
    "\n",
    "# Generate 'id_equipment' column\n",
    "try:\n",
    "    dfequip['id_equipment'] = range(1, len(dfequip) + 1)\n",
    "    logger.info(\"Added 'id_equipment'.\")\n",
    "    logger.debug(dfequip.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to add 'id_equipment' column.\", exc_info=True)\n",
    "\n",
    "# Rename 'name' column to 'equip_name'\n",
    "try:\n",
    "    dfequip = dfequip.rename(columns={'name': 'equip_name'})\n",
    "    logger.info(\"Renamed column 'name' to 'equip_name'.\")\n",
    "    logger.debug(dfequip.head())\n",
    "    logger.debug(dfequip.info())\n",
    "except KeyError as e:\n",
    "    logger.error(\"Failed to rename 'name' column because it is missing.\", exc_info=True)\n",
    "except Exception as e:\n",
    "    logger.error(\"An unexpected error occurred while renaming 'name' column.\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9dcf12-a873-4390-8826-c8f0fc377a21",
   "metadata": {},
   "source": [
    "## Dealing with reference_equip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a3dcfc-ee79-457e-8934-9d817addcd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the equipment column in dfstepclean dataframe because equipment is a list of equipments \n",
    "try:\n",
    "    dfstepclean_exploded = dfstepclean.explode(['equipment'])\n",
    "    dfstepclean_exploded = dfstepclean_exploded.reset_index(drop=True)\n",
    "    logger.info(\"'equipment' column in step dataframe exploded.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to explode 'equipment' column.\", exc_info=True)\n",
    "\n",
    "# Rename the exploded column to ing_name\n",
    "try:\n",
    "    dfstepclean_exploded = dfstepclean_exploded.rename(columns={'equipment': 'equip_name'})\n",
    "    logger.info(\"Renamed column 'equipment' to 'equip_name'.\")\n",
    "    logger.debug(dfstepclean_exploded.head())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to rename 'equipment' column\", exc_info=True)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# dealing with id_equipment in dfstepclean_exploded\n",
    "try:\n",
    "    # Create a mapping dictionary from dfequip\n",
    "    EQUIP_MAPPING = dfequip.set_index('equip_name')['id_equipment'].to_dict()\n",
    "    logger.info(\"'EQUIP_MAPPING' created\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to create 'EQUIP_MAPPING'.\", exc_info=True)\n",
    "\n",
    "# Function to map equipment names to id_equipment, handling missing values\n",
    "def map_with_none(equip_name, EQUIP_MAPPING):\n",
    "    \"\"\"Maps equipment names to id_equipment, handling missing values.\"\"\"\n",
    "    if pd.isna(equip_name):\n",
    "        return None  # Return None for missing equipment names\n",
    "    return EQUIP_MAPPING.get(equip_name, None)  # Use get() to avoid KeyError for missing keys\n",
    "\n",
    "# Apply the custom map function to fill the new column\n",
    "try:\n",
    "    dfstepclean_exploded['id_equipment'] = dfstepclean_exploded['equip_name'].apply(map_with_none, args=(EQUIP_MAPPING,))\n",
    "    logger.info(\"'id_equipment' column added according to the 'EQUIP_MAPPING'.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to add 'id_equipment' column.\", exc_info=True)\n",
    "\n",
    "# Convert id_equipment to integer type where applicable, keeping None values\n",
    "try:\n",
    "    dfstepclean_exploded['id_equipment'] = dfstepclean_exploded['id_equipment'].astype('Int64')\n",
    "    logger.info(\"'id_equipment' column converted to Integer.\")\n",
    "    logger.debug(dfstepclean_exploded.head())\n",
    "    logger.debug(dfstepclean_exploded.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to convert 'id_equipment' column to Integer.\", exc_info=True)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# dealing with id_recipe in dfstepclean_exploded\n",
    "\n",
    "try:\n",
    "    # Apply the custom map function to fill the new column\n",
    "    dfstepclean_exploded['id_recipe'] = dfstepclean_exploded['title'].apply(map_with_none, args=(RECIPE_MAPPING,))\n",
    "    logger.info(\"'id_recipe' column added according to the 'RECIPE_MAPPING'.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to add 'id_recipe' column.\", exc_info=True)\n",
    "    \n",
    "try:\n",
    "    # Convert id_recipe to integer type where applicable, keeping None values\n",
    "    dfstepclean_exploded['id_recipe'] = dfstepclean_exploded['id_recipe'].astype('Int64')\n",
    "    logger.info(\"'id_recipe' column converted to Integer.\")\n",
    "    logger.debug(dfstepclean_exploded.head())\n",
    "    logger.debug(dfstepclean_exploded.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to convert 'id_recipe' column to Integer.\", exc_info=True)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# creating the dfreference_equip dataframe\n",
    "try:\n",
    "    dfreference_equip = dfstepclean_exploded[['id_recipe', 'id_step', 'id_equipment']]\n",
    "    logger.info(\"Constructed reference_equip dataframe.\")\n",
    "    logger.debug(dfreference_equip.head())\n",
    "    logger.debug(dfreference_equip.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to create 'dfreference_equip' dataframe.\", exc_info=True)\n",
    "\n",
    "# Clean up dataframes\n",
    "try:\n",
    "    del dfstepclean_exploded\n",
    "    del dfstepclean\n",
    "    logger.info(\"Deleted intermediate dataframes.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"One or more intermediate dataframes were not found during deletion.\", exc_info=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166604a-699c-4646-bfa0-c51de4852f0e",
   "metadata": {},
   "source": [
    "## Dealing with dish types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914ecda-e29f-4454-a6b9-4031fdb6299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the required columns for the dish DataFrame\n",
    "dfdish_COLUMNS = ['dishTypes', 'title']\n",
    "# Construct the DataFrame\n",
    "try:\n",
    "    dfdish = pd.DataFrame(df, columns=dfdish_COLUMNS)\n",
    "    logger.info(f\"Extracted dish types for {len(dfdish)} recipes.\")\n",
    "    logger.debug(dfdish.head())\n",
    "    logger.debug(dfdish.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to construct dfdish DataFrame.\", exc_info=True)\n",
    "\n",
    "# Explode the 'dishTypes' column\n",
    "try:\n",
    "    dfALLdish_types = dfdish.explode('dishTypes').reset_index(drop=True)\n",
    "    logger.info(f\"'dishTypes' column exploded and there are {len(dfALLdish_types)} dish types.\")\n",
    "    logger.debug(dfALLdish_types.head())\n",
    "    logger.debug(dfALLdish_types.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to explode 'dishTypes' column.\", exc_info=True)\n",
    "\n",
    "# Copy the exploded DataFrame\n",
    "dfdish_type=dfALLdish_types.copy()\n",
    "\n",
    "# Delete 'title' column\n",
    "try:\n",
    "    dfdish_type.drop(columns=['title'], inplace=True)\n",
    "    logger.info(\"Deleted 'title' column.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to delete 'title' column.\", exc_info=True)\n",
    "\n",
    "try:\n",
    "    # Find duplicates\n",
    "    duplicates = dfdish_type[dfdish_type.duplicated()]\n",
    "    logger.debug(duplicates)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    if not duplicates.empty:\n",
    "        logger.warning(f\"Found {len(duplicates)} duplicate dish types. Removing duplicates.\")\n",
    "        dfdish_type = dfdish_type.drop_duplicates()\n",
    "        dfdish_type.reset_index(drop=True, inplace=True)\n",
    "        logger.info(f\"DataFrame now contains {len(dfdish_type)} dish types after removing duplicates.\")\n",
    "    else:\n",
    "        logger.info(\"No duplicates found.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred while finding or removing duplicates.\", exc_info=True)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Find Null values\n",
    "    none_rows = dfdish_type[dfdish_type['dishTypes'].isna()]\n",
    "    logger.debug(none_rows)\n",
    "\n",
    "    # Remove null values\n",
    "    if not none_rows.empty:\n",
    "        logger.warning(f\"Found {len(none_rows)} null dish types. Removing null values.\")\n",
    "        dfdish_type.dropna(inplace=True)\n",
    "        dfdish_type.reset_index(drop=True, inplace=True)\n",
    "        logger.info(f\"DataFrame now contains {len(dfdish_type)} dish types after removing null values.\")\n",
    "    else:\n",
    "        logger.info(\"No null values found.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred while finding or removing null values.\", exc_info=True)\n",
    "\n",
    "# Rename 'dishTypes' column to 'dish_type'\n",
    "try:\n",
    "    dfdish_type = dfdish_type.rename(columns={'dishTypes': 'dish_type'})\n",
    "    logger.info(\"Renamed column 'dishTypes' to 'dish_type'.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to rename 'dishTypes' column.\", exc_info=True)\n",
    "\n",
    "# Generate 'id_dish_type' column\n",
    "try:\n",
    "    dfdish_type['id_dish_type'] = range(1, len(dfdish_type) + 1)\n",
    "    logger.info(\"Added 'id_dish_type' column.\")\n",
    "    logger.debug(dfdish_type.head())\n",
    "    logger.debug(dfdish_type.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to add 'id_dish_type' column.\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56632e8-0671-441f-a0a3-d71b6f487f0b",
   "metadata": {},
   "source": [
    "## dfis_a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506ca6f-8d75-4efe-892d-e37ee3f5e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary from dfdish_type\n",
    "try:\n",
    "    DISH_MAPPING = dfdish_type.set_index('dish_type')['id_dish_type'].to_dict()\n",
    "    logger.info(\"'DISH_MAPPING' created.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to create DISH_MAPPING.\", exc_info=True)\n",
    "\n",
    "# Define a function to map dish types to id_dish_type, handling missing values\n",
    "def map_with_none(dish_type, mapping_dict):\n",
    "    \"\"\"Maps dish type names to id_dish_type, handling missing values.\"\"\"\n",
    "    if pd.isna(dish_type):\n",
    "        return None  # Return None for missing dish types\n",
    "    return mapping_dict.get(dish_type, None)  # Use get() to avoid KeyError for missing keys\n",
    "\n",
    "# Apply the custom map function to 'dishTypes' column\n",
    "try:\n",
    "    dfALLdish_types['id_dish_type'] = dfALLdish_types['dishTypes'].apply(map_with_none, args=(DISH_MAPPING,))\n",
    "    logger.info(\"'id_dish_type' column added according to the 'DISH_MAPPING'.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to map 'dishTypes' to 'id_dish_type'.\", exc_info=True)\n",
    "\n",
    "# Convert 'id_dish_type' to integer type where applicable, keeping None values\n",
    "try:\n",
    "    dfALLdish_types['id_dish_type'] = dfALLdish_types['id_dish_type'].astype('Int64')\n",
    "    logger.info(\"'id_dish_type' column converted to Integer.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to convert 'id_dish_type' to Integer.\", exc_info=True)\n",
    "\n",
    "# Apply the custom map function to 'title' column for 'id_recipe'\n",
    "try:\n",
    "    dfALLdish_types['id_recipe'] = dfALLdish_types['title'].apply(map_with_none, args=(RECIPE_MAPPING,))\n",
    "    logger.info(\"'id_recipe' column added according to the 'RECIPE_MAPPING'.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to map 'title' to 'id_recipe'.\", exc_info=True)\n",
    "\n",
    "# Convert 'id_recipe' to integer type where applicable, keeping None values\n",
    "try:\n",
    "    dfALLdish_types['id_recipe'] = dfALLdish_types['id_recipe'].astype('Int64')\n",
    "    logger.info(\"'id_recipe' column converted to Integer.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to convert 'id_recipe' to Integer.\", exc_info=True)\n",
    "\n",
    "# Log the DataFrame information\n",
    "logger.debug(dfALLdish_types.head())\n",
    "logger.debug(dfALLdish_types.info())\n",
    "\n",
    "# Delete 'dishTypes' and 'title' columns\n",
    "try:\n",
    "    dfALLdish_types.drop(columns=['dishTypes', 'title'], inplace=True)\n",
    "    logger.info(\"Deleted columns 'dishTypes' and 'title'.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to delete columns 'dishTypes' or 'title'.\", exc_info=True)\n",
    "\n",
    "# Copy the DataFrame and delete the original\n",
    "try:\n",
    "    dfis_a = dfALLdish_types.copy()\n",
    "    del dfALLdish_types\n",
    "    logger.info(\"Copied dfALLdish_types to dfis_a and deleted dfALLdish_types.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to copy or delete dfALLdish_types.\", exc_info=True)\n",
    "\n",
    "# Log the final DataFrame information\n",
    "logger.debug(dfis_a.head())\n",
    "logger.debug(dfis_a.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de99352d-a089-4003-8e0b-aab5b4a33cfe",
   "metadata": {},
   "source": [
    "## Dealing with cuisines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55062e80-ff48-41b1-ab70-488218a987b6",
   "metadata": {},
   "source": [
    "### extracting needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3178b7-f432-4b30-9eb1-46efbdf1c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the required columns for the cuisines DataFrame\n",
    "dfcuisines_COLUMNS = ['cuisines', 'title']\n",
    "\n",
    "# Construct the DataFrame\n",
    "try:\n",
    "    dfALLcuisines = pd.DataFrame(df, columns=dfcuisines_COLUMNS)\n",
    "    logger.info(f\"Extracted cuisines for {len(dfALLcuisines)} recipes.\")\n",
    "    logger.debug(dfALLcuisines.head())\n",
    "    logger.debug(dfALLcuisines.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to construct dfALLcuisines DataFrame.\", exc_info=True)\n",
    "\n",
    "# Explode the 'cuisines' column\n",
    "try:\n",
    "    dfALLcuisines = dfALLcuisines.explode('cuisines').reset_index(drop=True)\n",
    "    logger.info(f\"'cuisines' column exploded and contains {len(dfALLcuisines)} cuisines.\")\n",
    "    logger.debug(dfALLcuisines.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to explode 'cuisines' column.\", exc_info=True)\n",
    "\n",
    "# Copy the exploded DataFrame\n",
    "dfcuisine=dfALLcuisines.copy()\n",
    "\n",
    "# Delete 'title' column\n",
    "try:\n",
    "    dfcuisine.drop(columns=['title'], inplace=True)\n",
    "    logger.info(\"Deleted 'title' column.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to delete 'title' column.\", exc_info=True)\n",
    "\n",
    "try:\n",
    "    # Find duplicates\n",
    "    duplicates = dfcuisine[dfcuisine.duplicated()]\n",
    "    logger.debug(duplicates)\n",
    "\n",
    "    # Remove duplicates\n",
    "    if not duplicates.empty:\n",
    "        logger.warning(f\"Found {len(duplicates)} duplicate cuisines. Removing duplicates.\")\n",
    "        dfcuisine = dfcuisine.drop_duplicates()\n",
    "        dfcuisine.reset_index(drop=True, inplace=True)\n",
    "        logger.info(f\"DataFrame now contains {len(dfcuisine)} cuisines after removing duplicates.\")\n",
    "    else:\n",
    "        logger.info(\"No duplicates found.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred while finding or removing duplicates.\", exc_info=True)\n",
    "\n",
    "try:\n",
    "    # Find Null values\n",
    "    none_rows = dfcuisine[dfcuisine['cuisines'].isna()]\n",
    "    logger.debug(none_rows)\n",
    "\n",
    "    # Remove null values\n",
    "    if not none_rows.empty:\n",
    "        logger.warning(f\"Found {len(none_rows)} null cuisine values. Removing null values.\")\n",
    "        dfcuisine.dropna(inplace=True)\n",
    "        dfcuisine.reset_index(drop=True, inplace=True)\n",
    "        logger.info(f\"DataFrame now contains {len(dfcuisine)} cuisines after removing null values.\")\n",
    "    else:\n",
    "        logger.info(\"No null values found.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred while finding or removing null values.\", exc_info=True)\n",
    "\n",
    "# Rename 'cuisines' column to 'recipe_cuisine'\n",
    "try:\n",
    "    dfcuisine = dfcuisine.rename(columns={'cuisines': 'recipe_cuisine'})\n",
    "    logger.info(\"Renamed column 'cuisines' to 'recipe_cuisine'.\")\n",
    "    logger.debug(dfcuisine.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to rename 'cuisines' column.\", exc_info=True)\n",
    "\n",
    "# Generate 'id_cuisine' column\n",
    "try:\n",
    "    dfcuisine['id_cuisine'] = range(1, len(dfcuisine) + 1)\n",
    "    logger.info(\"Added 'id_cuisine' column.\")\n",
    "    logger.debug(dfcuisine.head())\n",
    "    logger.debug(dfcuisine.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to add 'id_cuisine' column.\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e3f24-00d6-4b09-9f30-468ba20641d6",
   "metadata": {},
   "source": [
    "### dfbelongs dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647271a1-63dc-4b23-ac5a-a7ef8697613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the CUISINE_MAPPING dictionary is created\n",
    "try:\n",
    "    CUISINE_MAPPING = dfcuisine.set_index('recipe_cuisine')['id_cuisine'].to_dict()\n",
    "    logger.info(\"'CUISINE_MAPPING' created\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to create 'CUISINE_MAPPING'.\", exc_info=True)\n",
    "\n",
    "# Define a function to map recipe cuisines to id_cuisine, handling missing values\n",
    "def map_with_none(recipe_cuisine, CUISINE_MAPPING):\n",
    "    \"\"\"Maps recipes cuisines to id_cuisine, handling missing values.\"\"\"\n",
    "    if pd.isna(recipe_cuisine):\n",
    "        return None  # Return None for missing cuisines\n",
    "    return CUISINE_MAPPING.get(recipe_cuisine, None)  # Use get() to avoid KeyError for missing keys\n",
    "\n",
    "# Apply the custom map function to add 'id_cuisine' column\n",
    "try:\n",
    "    dfALLcuisines['id_cuisine'] = dfALLcuisines['cuisines'].apply(map_with_none, args=(CUISINE_MAPPING,))\n",
    "    logger.info(\"'id_cuisine' column added according to the 'CUISINE_MAPPING'.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to map 'id_cuisine' to 'cuisines'.\", exc_info=True)\n",
    "\n",
    "# Convert id_cuisine to integer type where applicable, keeping None values\n",
    "try:\n",
    "    dfALLcuisines['id_cuisine'] = dfALLcuisines['id_cuisine'].astype('Int64')\n",
    "    logger.info(\"'id_cuisine' column converted to Integer \")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to convert 'id_cuisine' to Integer.\", exc_info=True)\n",
    "\n",
    "\n",
    "# Apply the custom map function to add 'id_recipe' column\n",
    "try:\n",
    "    dfALLcuisines['id_recipe'] = dfALLcuisines['title'].apply(map_with_none, args=(RECIPE_MAPPING,))\n",
    "    logger.info(\"'id_recipe' column added according to the 'RECIPE_MAPPING'.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to map 'id_recipe' to 'title'.\", exc_info=True)\n",
    "\n",
    "# Convert id_recipe to integer type where applicable, keeping None values\n",
    "try:\n",
    "    dfALLcuisines['id_recipe'] = dfALLcuisines['id_recipe'].astype('Int64')\n",
    "    logger.info(\"'id_recipe' column converted to Integer.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to convert 'id_recipe' to Integer.\", exc_info=True)\n",
    "\n",
    "# Log the DataFrame head and info\n",
    "logger.debug(dfALLcuisines.head())\n",
    "logger.debug(dfALLcuisines.info())\n",
    "\n",
    "# Drop 'cuisines' and 'title' columns from dfcuisines\n",
    "try:\n",
    "    dfALLcuisines.drop(columns=['cuisines', 'title'], inplace=True)\n",
    "    logger.info(\"Deleted 'cuisines' and 'title' columns.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to delete 'cuisines' or 'title' columns.\", exc_info=True)\n",
    "\n",
    "# Copy dfcuisines to dfbelongs and delete dfcuisines\n",
    "try:\n",
    "    dfbelongs = dfALLcuisines.copy()\n",
    "    logger.info(\"Copied dfALLcuisines to dfbelongs.\")\n",
    "    \n",
    "    del dfALLcuisines\n",
    "    logger.info(\"Deleted dfALLcuisines.\")\n",
    "\n",
    "    logger.debug(dfbelongs.head())\n",
    "    logger.debug(dfbelongs.info())\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to copy or delete dfcuisines.\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8135aa8c-ffe2-4aa7-af86-75c8d55151c6",
   "metadata": {},
   "source": [
    "## Transform Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02c5f2-8aff-4b87-8dca-5b77a107e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Displaying transforamtion recap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e0fbc-20c7-446e-b57b-7835da0a9365",
   "metadata": {},
   "source": [
    "### recipe table\n",
    "(id_recipe INT, recipe_title VARCHAR(50), ready_min INT, summary VARCHAR(2000), servings INT, is_cheap LOGICAL, price_per_serving DOUBLE, is_vegetarian LOGICAL, is_vegan LOGICAL, is_glutenFree LOGICAL, is_dairyFree LOGICAL, is_healthy LOGICAL, is_sustainable LOGICAL, is_lowFodmap LOGICAL, is_Popular LOGICAL, license VARCHAR(20), source_url VARCHAR(100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627cf49-4ce9-4c64-b59e-680b7987be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(dfrecipes.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44bf644-7924-45c2-b67c-bbb3746a6e32",
   "metadata": {},
   "source": [
    "### Ingredients table\n",
    "(id_ingredient INT, ing_name VARCHAR(50), consistency VARCHAR(20), aisle VARCHAR(20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1734fd-cc20-4f42-8b9d-d2fa5c55f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(dfIng.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfa020-68bd-44b2-a18e-88b5be90796a",
   "metadata": {},
   "source": [
    "### reference_ing table\n",
    "(#id_recipe, #id_ingredient, measure VARCHAR(50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254fc349-9807-41a8-a87d-89e0f079a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(dfreference_ing.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218949e7-0cf7-48a5-b063-26e2644dcd0d",
   "metadata": {},
   "source": [
    "### Equipment table\n",
    "(id_equipment INT, equip_name VARCHAR(50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59fdaa-d226-4b8e-a57a-831ede5fe0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(dfequip.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bae6b6-41e7-485a-911e-1e8bb06df22c",
   "metadata": {},
   "source": [
    "### Instructions table\n",
    "Instruction = (id_instruction INT, #id_recipe);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e40f27-6f6b-4e63-a5c5-6e4c82ab01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(dfIns.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48ea81-8a76-4e30-b834-4d6220230591",
   "metadata": {},
   "source": [
    "### steps table\n",
    "(id_step INT, step VARCHAR(8000), number INT, length VARCHAR(50), #id_instruction);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd064b9-1fc4-4d11-8a98-57b721e21c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(dfstep_final.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5b76e-f251-4f28-a25d-d6f5f00346d4",
   "metadata": {},
   "source": [
    "### reference_equip table\n",
    "(#id_recipe, #id_step, #id_equipment);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c5eb7-825f-451a-b4da-c757b296bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(dfreference_equip.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dfcbf-f79b-4b55-bd69-3caf5dcb50a0",
   "metadata": {},
   "source": [
    "### dish_type table\n",
    "(id_dish_type INT, dish_type VARCHAR(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7aa308-5a88-42f7-82b8-31d102287bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(dfdish_type.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f2f96-dd2a-43fc-9107-78dca4d9b3d5",
   "metadata": {},
   "source": [
    "### is_a  table\n",
    "(#id_recipe, #id_dish_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b8163-a100-42dd-b713-c7c8182ae22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(dfis_a.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2d456-24e4-452b-a595-daa9c2a940a1",
   "metadata": {},
   "source": [
    "### cuisine table\n",
    "(id_cuisine INT, recipe_cuisine VARCHAR(50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380de1a-a964-44a3-81b0-a3cf4e26fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(dfcuisine.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d41f4b-7b4d-4cb2-8d2f-6837bb8f3efe",
   "metadata": {},
   "source": [
    "### belongs table\n",
    "(#id_recipe, #id_cuisine);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d0664-1d66-404b-ae05-1abe45d770e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(dfbelongs.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2159e-7554-4907-bc61-b02bf4d11743",
   "metadata": {},
   "source": [
    "# load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a824d2e0-108e-4301-80df-8b2519e3ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configuration\n",
    "DB_HOST='10.0.2.15' # Database host IP address\n",
    "DB_PORT= 5432 # Database port number\n",
    "DB_NAME='recipe_etl' # Name of the database\n",
    "DB_USER='maryem' # Database user name\n",
    "DB_PASSWORD='HelloWorld' # Database user password\n",
    "\n",
    "try:\n",
    "    # Establish connection to the database\n",
    "    conn = psycopg2.connect(\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "            dbname=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD\n",
    "        )\n",
    "    logger.info(f\"Connected to {DB_NAME}\")\n",
    "except psycopg2.Error as e:\n",
    "    logger.error(f\"Error: Could not make connection to the database {DB_NAME}\")\n",
    "    logger.error(e)\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070773a6-bb66-4973-97d2-fa461f0b935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceed only if the connection was successful\n",
    "if conn:\n",
    "    try:\n",
    "        # Obtain a cursor to execute queries\n",
    "        cur = conn.cursor()\n",
    "        logger.info(f\"Cursor obtained for the database {DB_NAME}\")\n",
    "    except psycopg2.Error as e:\n",
    "        logger.error(f\"Error: Could not get cursor for the database {DB_NAME}\")\n",
    "        logger.error(e)\n",
    "        cur = None\n",
    "\n",
    "    if cur:\n",
    "        # Helper function to execute an insert query and check the row count\n",
    "        def execute_insert_and_check(query, values, table_name):\n",
    "            try:\n",
    "                cur.execute(query, values)\n",
    "                conn.commit()\n",
    "                \n",
    "                # Check if the table is empty after insertion\n",
    "                cur.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "                row_count = cur.fetchone()[0]\n",
    "                \n",
    "                if row_count == 0:\n",
    "                    logger.error(f\"The {table_name} table is empty after insertion attempts.\")\n",
    "                    raise ValueError(f\"The {table_name} table is empty after insertion attempts.\")\n",
    "                else:\n",
    "                    logger.info(f\"Values inserted into {table_name} successfully\")\n",
    "            except psycopg2.Error as e:\n",
    "                logger.error(f\"Error in inserting values into {table_name}: {e}\")\n",
    "                conn.rollback()\n",
    "                raise\n",
    "            except ValueError as ve:\n",
    "                logger.error(ve)\n",
    "                raise\n",
    "\n",
    "        # Insert into Recipe table\n",
    "        try:\n",
    "            for index, row in dfrecipes.iterrows():\n",
    "                execute_insert_and_check(\"\"\"\n",
    "                    INSERT INTO Recipe (id_recipe, recipe_title, ready_min, summary, servings, is_cheap, price_per_serving, \n",
    "                    is_vegetarian, is_vegan, is_glutenFree, is_dairyFree, is_healthy, is_sustainable, is_lowFodmap, \n",
    "                    is_Popular, license, source_url)\n",
    "                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\"\"\", (\n",
    "                    int(row['id_recipe']),\n",
    "                    row['recipe_title'],\n",
    "                    int(row['ready_min']),\n",
    "                    row['summary'],\n",
    "                    int(row['servings']),\n",
    "                    bool(row['is_cheap']),\n",
    "                    float(row['price_per_serving']),\n",
    "                    bool(row['is_vegetarian']),\n",
    "                    bool(row['is_vegan']),\n",
    "                    bool(row['is_glutenFree']),\n",
    "                    bool(row['is_dairyFree']),\n",
    "                    bool(row['is_healthy']),\n",
    "                    bool(row['is_sustainable']),\n",
    "                    bool(row['is_lowFodmap']),\n",
    "                    bool(row['is_Popular']),\n",
    "                    row['license'],\n",
    "                    row['source_url'],\n",
    "                ), 'Recipe')\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to insert into Recipe table\")\n",
    "\n",
    "        # Insert into Instruction table\n",
    "        try:\n",
    "            for index, row in dfIns.iterrows():\n",
    "                execute_insert_and_check(\"\"\"\n",
    "                    INSERT INTO Instruction (id_instruction, id_recipe)\n",
    "                    VALUES (%s, %s);\"\"\", (\n",
    "                    int(row['instruction_id']),\n",
    "                    int(row['id_recipe'])\n",
    "                ), 'Instruction')\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to insert into Instruction table\")\n",
    "\n",
    "        # Insert into Ingredient table\n",
    "        try:\n",
    "            for index, row in dfIng.iterrows():\n",
    "                execute_insert_and_check(\"\"\"\n",
    "                    INSERT INTO Ingredient (id_ingredient, ing_name, consistency, aisle)\n",
    "                    VALUES (%s, %s, %s, %s);\"\"\", (\n",
    "                    int(row['id_ingredient']),\n",
    "                    row['ing_name'],\n",
    "                    row['consistency'],\n",
    "                    row['aisle']\n",
    "                ), 'Ingredient')\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to insert into Ingredient table\")\n",
    "\n",
    "        # Insert into Step table\n",
    "        try:\n",
    "            for index, row in dfstep_final.iterrows():\n",
    "                execute_insert_and_check(\"\"\"\n",
    "                    INSERT INTO Step (id_step, step, number, length, id_instruction)\n",
    "                    VALUES (%s, %s, %s, %s, %s);\"\"\", (\n",
    "                    int(row['id_step']),\n",
    "                    row['step'],\n",
    "                    int(row['number']),\n",
    "                    row['length'],\n",
    "                    int(row['instruction_id'])\n",
    "                ), 'Step')\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to insert into Step table\")\n",
    "\n",
    "        # Insert into Equipment table\n",
    "        try:\n",
    "            for index, row in dfequip.iterrows():\n",
    "                execute_insert_and_check(\"\"\"\n",
    "                    INSERT INTO Equipment (id_equipment, equip_name)\n",
    "                    VALUES (%s, %s);\"\"\", (\n",
    "                    int(row['id_equipment']),\n",
    "                    row['equip_name']\n",
    "                ), 'Equipment')\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to insert into Equipment table\")\n",
    "\n",
    "        # Insert into Dish table\n",
    "        try:\n",
    "            for index, row in dfdish_type.iterrows():\n",
    "                execute_insert_and_check(\"\"\"\n",
    "                    INSERT INTO Dish (id_dish_type, dish_type)\n",
    "                    VALUES (%s, %s);\"\"\", (\n",
    "                    int(row['id_dish_type']),\n",
    "                    row['dish_type']\n",
    "                ), 'Dish')\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to insert into Dish table\")\n",
    "\n",
    "        # Insert into Cuisine table\n",
    "        try:\n",
    "            for index, row in dfcuisine.iterrows():\n",
    "                execute_insert_and_check(\"\"\"\n",
    "                    INSERT INTO Cuisine (id_cuisine, recipe_cuisine)\n",
    "                    VALUES (%s, %s);\"\"\", (\n",
    "                    int(row['id_cuisine']),\n",
    "                    row['recipe_cuisine']\n",
    "                ), 'Cuisine')\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to insert into Cuisine table\")\n",
    "\n",
    "        # Insert into reference_ing table\n",
    "        try:\n",
    "            for index, row in dfreference_ing.iterrows():\n",
    "                execute_insert_and_check(\"\"\"\n",
    "                    INSERT INTO reference_ing (id_recipe, id_ingredient, measure)\n",
    "                    VALUES (%s, %s, %s);\"\"\", (\n",
    "                    int(row['id_recipe']),\n",
    "                    int(row['id_ingredient']),\n",
    "                    row['measure']\n",
    "                ), 'reference_ing')\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to insert into reference_ing table\")\n",
    "\n",
    "        # Insert into reference_equip table\n",
    "        dfreference_equip = dfreference_equip.replace({pd.NA: np.nan})\n",
    "        try:\n",
    "            for index, row in dfreference_equip.iterrows():\n",
    "                id_equipment = None if pd.isna(row['id_equipment']) else int(row['id_equipment'])\n",
    "                execute_insert_and_check(\"\"\"\n",
    "                    INSERT INTO reference_equip (id_recipe, id_step, id_equipment)\n",
    "                    VALUES (%s, %s, %s);\"\"\", (\n",
    "                    int(row['id_recipe']),\n",
    "                    int(row['id_step']),\n",
    "                    id_equipment\n",
    "                ), 'reference_equip')\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to insert into reference_equip table\")\n",
    "\n",
    "        # Insert into is_a table\n",
    "        dfis_a = dfis_a.replace({pd.NA: np.nan})\n",
    "        try:\n",
    "            for index, row in dfis_a.iterrows():\n",
    "                id_dish_type = None if pd.isna(row['id_dish_type']) else int(row['id_dish_type'])\n",
    "                execute_insert_and_check(\"\"\"\n",
    "                    INSERT INTO is_a (id_recipe, id_dish_type)\n",
    "                    VALUES (%s, %s);\"\"\", (\n",
    "                    int(row['id_recipe']),\n",
    "                    id_dish_type\n",
    "                ), 'is_a')\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to insert into is_a table\")\n",
    "\n",
    "        # Insert into belongs table\n",
    "        dfbelongs = dfbelongs.replace({pd.NA: np.nan})\n",
    "        try:\n",
    "            for index, row in dfbelongs.iterrows():\n",
    "                id_cuisine = None if pd.isna(row['id_cuisine']) else int(row['id_cuisine'])\n",
    "                execute_insert_and_check(\"\"\"\n",
    "                    INSERT INTO belongs (id_recipe, id_cuisine)\n",
    "                    VALUES (%s, %s);\"\"\", (\n",
    "                    int(row['id_recipe']),\n",
    "                    id_cuisine\n",
    "                ), 'belongs')\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to insert into belongs table\")\n",
    "        \n",
    "        finally:\n",
    "            cur.close()\n",
    "    else:\n",
    "        logger.error(\"Cursor could not be obtained. Exiting the program.\")\n",
    "    conn.close()\n",
    "else:\n",
    "    logger.error(\"Connection to the database could not be established. Exiting the program.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9706d5-778e-45a4-a893-bcaed283e120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
